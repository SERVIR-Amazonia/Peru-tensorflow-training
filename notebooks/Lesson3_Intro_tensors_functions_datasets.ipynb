{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qFdPvlXBOdUN"
      },
      "source": [
        "# Introduction to Tensors, TensorFlow Functions and TensorFlow Datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pVaq-F6yw97K"
      },
      "source": [
        "In this tutorial, we will learn about some key aspects of TensorFlow. First we will start by discussing tensors, tensorflow's fundamental data type. Next, we'll cover `tf.function` and when to use it for performance optimization and model portability. Lastly we will discuss `tf.data.Dataset` methods and how to create tensor-formatted datasets for efficient datapipelines that operate on large datasets.\n",
        "\n",
        "This is an adaptation from [Introduction to Tensors](https://www.tensorflow.org/guide/tensor), [TensorFlow Functions](https://colab.research.google.com/github/tensorflow/docs/blob/master/site/en/guide/function.ipynb#scrollTo=HQrG5_kOiKl_), and [tf.data.Dataset API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "AL2hzxorJiWy"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DRK5-9EpYbzG"
      },
      "source": [
        "## Basics of tensors\n",
        "\n",
        "Let's create some basic tensors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQ3s2J8Vgowq"
      },
      "source": [
        "A tensor is a multi-dimensional array with a consistent type (known as a `dtype`).  All supported `dtypes` can be seen with `tf.dtypes.DType`.\n",
        "\n",
        "Tensors are similar to [NumPy](https://numpy.org/devdocs/user/quickstart.html)  `np.arrays`.\n",
        "\n",
        "Tensors are immutable, just like Python numbers and strings. The contents of a tensor can never be updated; only new ones can be created.\n",
        "\n",
        "Note that we will use `tf.constant` regularly, and sometimes you'll see `tf.variable` for similarly looking assignments. The difference between the two in TensorFlow usage is that, the value assigned to a constant cannot be changed in the future; it is static and the initialization is with a value, not an operation. Whereas for variable, the initialization can be a value or an operation, both of which can change. In the context of machine learning, we would want to assign the loss value as a variable as it changes with each epoch, whereas we would want to assign fixed hyper-parameters like batch size as a constant. Training samples (images for example) are also converted to tensors as constants as they don't change.\n",
        "\n",
        "Can you guess where else in the machine learning life cycle we would want to use variables instead of constants and vice versa?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NvSAbowVVuRr"
      },
      "source": [
        "### Significance of Tensor shapes\n",
        "\n",
        "A tensor has a shape.  Some key terms to know:\n",
        "\n",
        "* **Shape**: Describes the number of axes and length (number of elements) in each axis of a tensor.\n",
        "* **Rank**: Determined by the number of axes in a tensor.  \n",
        "* **Axis** or **Dimension**: A dimension of a tensor.\n",
        "* **Size**: The total number of elements in a tensor. This can be expressed as the product of the shape vector's elements, e.g. a tensor shape of (256, 256, 3) will have a size of 196608."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uSHRFT6LJbxq"
      },
      "source": [
        "A \"scalar\" is a \"rank-0\" tensor. It contains a single value and no \"axes\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d5JcgLFR6gHv"
      },
      "outputs": [],
      "source": [
        "# This will be an int32 tensor by default; see \"dtypes\" below.\n",
        "rank_0_tensor = tf.constant(4)\n",
        "print(rank_0_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tdmPAn9fWYs5"
      },
      "source": [
        "A \"vector\" is a \"rank-1\" tensor and contains something like a list of values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oZos8o_R6oE7"
      },
      "outputs": [],
      "source": [
        "# Let's make this a float tensor.\n",
        "rank_1_tensor = tf.constant([2.0, 3.0, 4.0])\n",
        "print(rank_1_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G3IJG-ug_H4u"
      },
      "source": [
        "A \"matrix\" is a \"rank-2\" tensor and it has at least two axes. This is what a single channel image turns out to be in tensor format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cnOIA_xb6u0M"
      },
      "outputs": [],
      "source": [
        "# If you want to be specific, you can set the dtype (see below) at creation time\n",
        "rank_2_tensor = tf.constant([[1, 2],\n",
        "                             [3, 4],\n",
        "                             [5, 6]], dtype=tf.float16)\n",
        "print(rank_2_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "19m72qEPkfxi"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>A scalar, shape: <code>[]</code></th>\n",
        "  <th>A vector, shape: <code>[3]</code></th>\n",
        "  <th>A matrix, shape: <code>[3, 2]</code></th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/scalar.png?raw=1\" alt=\"A scalar, the number 4\" />\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "   <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/vector.png?raw=1\" alt=\"The line with 3 sections, each one containing a number.\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/matrix.png?raw=1\" alt=\"A 3x2 grid, with each cell containing a number.\">\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fjFvzcn4_ehD"
      },
      "source": [
        "Tensors can feature more axes, as is required for representation of multi-channel images. For example, we can have a tensor with three axes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sesW7gw6JkXy"
      },
      "outputs": [],
      "source": [
        "# There can be an arbitrary number of\n",
        "# axes (sometimes called \"dimensions\")\n",
        "\n",
        "rank_3_tensor = tf.constant([\n",
        "  [[0, 1, 2, 3, 4],\n",
        "   [5, 6, 7, 8, 9]],\n",
        "  [[10, 11, 12, 13, 14],\n",
        "   [15, 16, 17, 18, 19]],\n",
        "  [[20, 21, 22, 23, 24],\n",
        "   [25, 26, 27, 28, 29]],])\n",
        "\n",
        "print(rank_3_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rM2sTGIkoE3S"
      },
      "source": [
        "A tensor with more than two axes can be visualized in several ways."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NFiYfNMMhDgL"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=3>A 3-axis tensor, shape: <code>[3, 2, 5]</code></th>\n",
        "<tr>\n",
        "<tr>\n",
        "  <td>\n",
        "   <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/3-axis_numpy.png?raw=1\"/>\n",
        "  </td>\n",
        "  <td>\n",
        "   <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/3-axis_front.png?raw=1\"/>\n",
        "  </td>\n",
        "\n",
        "  <td>\n",
        "   <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/3-axis_block.png?raw=1\"/>\n",
        "  </td>\n",
        "</tr>\n",
        "\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Something like a small 3 channel image\n",
        "\n",
        "rank_3_tensor = tf.constant([\n",
        "  [[0, 1, 2],\n",
        "   [3, 4, 5],\n",
        "   [6, 7, 8]],\n",
        "  [[9, 10, 11],\n",
        "   [12, 13, 14],\n",
        "   [15, 16, 17]],\n",
        "  [[18, 19, 20],\n",
        "   [21, 22, 23],\n",
        "   [24, 25, 26]]])\n",
        "\n",
        "print(rank_3_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VFOyG2tn8LhW"
      },
      "source": [
        "### Interpreting tensor shapes\n",
        "`tf.TensorShape` objects describe some key properties of tensor shapes which help us understand the dimensionality of the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "RyD3yewUKdnK"
      },
      "outputs": [],
      "source": [
        "rank_4_tensor = tf.zeros([3, 2, 4, 5]) # A fairly complex shape as an example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oTZZW9ziq4og"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th colspan=2>A rank-4 tensor, shape: <code>[3, 2, 4, 5]</code></th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/shape.png?raw=1\" alt=\"A tensor shape is like a vector.\">\n",
        "    <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/4-axis_block.png?raw=1\" alt=\"A 4-axis tensor\">\n",
        "  </td>\n",
        "  </tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MHm9vSqogsBk"
      },
      "outputs": [],
      "source": [
        "print(\"Type of every element:\", rank_4_tensor.dtype)\n",
        "print(\"Number of axes:\", rank_4_tensor.ndim)\n",
        "print(\"Rank: \", tf.rank(rank_4_tensor))\n",
        "print(\"Shape of tensor:\", rank_4_tensor.shape)\n",
        "print(\"Elements along axis 0 of tensor:\", rank_4_tensor.shape[0])\n",
        "print(\"Elements along the last axis of tensor:\", rank_4_tensor.shape[-1])\n",
        "print(\"Total number of elements (3*2*4*5): \", tf.size(rank_4_tensor).numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bQmE_Vx5JilS"
      },
      "source": [
        "Axes of a tensor follow a global to local ordering. For example, in machine learning we often talk about batches. For a given batch, the dimension of the batch is ordered first, followed by the batch element's spatial dimensions (height, width) and then the channels (referred to as features in the below diagram).\n",
        "\n",
        "<table>\n",
        "<tr>\n",
        "<th>Typical axis order</th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/shape2.png?raw=1\" alt=\"Keep track of what each axis is. A 4-axis tensor might be: Batch, Width, Height, Channels\">\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnz19F0ocEKD"
      },
      "source": [
        "### Types of tensors often used\n",
        "Most tensors we encounter in computer vision contain floats and integers, but tensors can also represent other types, such as:\n",
        "\n",
        "* complex numbers\n",
        "* strings\n",
        "\n",
        "TensorFlow requires a `tf.Tensor` to be \"rectangular,\" which means that every element along an axis is the same size. There are exceptions to this (ragged and sparse tensors, but those are more rare)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SDC7OGeAIJr8"
      },
      "source": [
        "We can calculate basic mathematical operations on tensors, such as addition, element-wise multiplication, and matrix multiplication."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-DTkjwDOIIDa"
      },
      "outputs": [],
      "source": [
        "a = tf.constant([[1, 2],\n",
        "                 [3, 4]])\n",
        "b = tf.constant([[1, 1],\n",
        "                 [1, 1]]) # Could have also said `tf.ones([2,2], dtype=tf.int32)`\n",
        "\n",
        "print(tf.add(a, b), \"\\n\") # element-wise addition\n",
        "print(tf.multiply(a, b), \"\\n\") # element-wise multiplication\n",
        "print(tf.matmul(a, b), \"\\n\") # matrix multiplication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2smoWeUz-N2q"
      },
      "outputs": [],
      "source": [
        "print(a + b, \"\\n\") # element-wise addition\n",
        "print(a * b, \"\\n\") # element-wise multiplication\n",
        "print(a @ b, \"\\n\") # matrix multiplication"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3_vIAl2JPVc"
      },
      "source": [
        "Tensors are used in many types of operations (or \"Ops\"), such as common machine learning operations like `tf.math.argmax` and `tf.nn.softmax`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gp4WUYzGIbnv"
      },
      "outputs": [],
      "source": [
        "c = tf.constant([[4.0, 5.0], [10.0, 1.0]])\n",
        "\n",
        "# Find the largest value\n",
        "print(tf.reduce_max(c))\n",
        "# Find the index of the largest value\n",
        "print(tf.math.argmax(c))\n",
        "# Compute the softmax\n",
        "print(tf.nn.softmax(c))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWAc0U8OZwNb"
      },
      "source": [
        "NumPy arrays can be created from tensors using either the `np.array` or `tensor.numpy` methods:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c6Taz2gIaZeo"
      },
      "outputs": [],
      "source": [
        "rank_2_tensor.numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "J5u6_6ZYaS7B"
      },
      "outputs": [],
      "source": [
        "np.array(rank_2_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0MNM-q7-MZLz"
      },
      "source": [
        "Generally, where tensors are expected, TensorFlow functions additionally accept anything that can be converted to a `Tensor` using `tf.convert_to_tensor`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_wch0N8xNEt-"
      },
      "outputs": [],
      "source": [
        "tf.convert_to_tensor([1,2,3]) # List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ngqIeWYeNJVI"
      },
      "outputs": [],
      "source": [
        "tf.reduce_max([1,2,3]) # List"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ThVMxqbVNOq3"
      },
      "outputs": [],
      "source": [
        "tf.reduce_max(np.array([1,2,3])) # NumPy array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skOqX5NBKJi5"
      },
      "source": [
        "### Variables\n",
        "\n",
        "As previously mentioned, normal `tf.Tensor`s are immutable. However, some objects will change over time, like the trainable weights in a machine learning model. We want to store these values, so in order to do that, we need a tensor that can change. Enter the `tf.Variable`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "u4Ax2DGhKuUv"
      },
      "outputs": [],
      "source": [
        "var = tf.Variable([0.0, 0.0, 0.0]) # example rank 1 variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ytNjVeFTK4EE"
      },
      "outputs": [],
      "source": [
        "var.assign([1, 2, 3]) # change / re-assign the contents of the variable"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "27F_PELQK8hi"
      },
      "outputs": [],
      "source": [
        "var.assign_add([1, 1, 1]) # change the contents of the variable using an operation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FlPoVvJS75Bb"
      },
      "source": [
        "## Indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "apOkCKqCZIZu"
      },
      "source": [
        "### Single-axis indexing\n",
        "\n",
        "In TensorFlow, standard Python indexing rules apply. Indexing tensors looks similar to [indexing a list in Python](https://docs.python.org/3/tutorial/introduction.html#strings). The same applied for NumPy-style indexing.\n",
        "\n",
        "* indexes begin at `0`\n",
        "* negative indices count backwards from the end\n",
        "* colons, `:`, are used for slices: `start:stop:step`\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SQ-CrJxLXTIM"
      },
      "outputs": [],
      "source": [
        "rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])\n",
        "print(rank_1_tensor.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2cBo-BJeUSLy"
      },
      "source": [
        "When using a scalar to get an element in the tensor, the axis is removed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n6tqHciOWMt5"
      },
      "outputs": [],
      "source": [
        "print(\"First:\", rank_1_tensor[0].numpy())\n",
        "print(\"Second:\", rank_1_tensor[1].numpy())\n",
        "print(\"Last:\", rank_1_tensor[-1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qJLHU_a2XwpG"
      },
      "source": [
        "When a slice is indexed using `:` the axis is retained:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "giVPPcfQX-cu"
      },
      "outputs": [],
      "source": [
        "print(\"Everything:\", rank_1_tensor[:].numpy())\n",
        "print(\"Before 4:\", rank_1_tensor[:4].numpy())\n",
        "print(\"From 4 to the end:\", rank_1_tensor[4:].numpy())\n",
        "print(\"From 2, before 7:\", rank_1_tensor[2:7].numpy())\n",
        "print(\"Every other item:\", rank_1_tensor[::2].numpy())\n",
        "print(\"Reversed:\", rank_1_tensor[::-1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elDSxXi7X-Bh"
      },
      "source": [
        "### Multi-axis indexing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cgk0uRUYZiai"
      },
      "source": [
        "When working with higher ranks of tensors, we may have to use multiple indices.\n",
        "\n",
        "To do this, we treat each axis independently with the exact same rules as in the single-axis case."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Tc5X_WlsZXmd"
      },
      "outputs": [],
      "source": [
        "print(rank_2_tensor.numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w07U9vq5ipQk"
      },
      "source": [
        "If we pass an integer for each index for each axis, a scalar is returned."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PvILXc1PjqTM"
      },
      "outputs": [],
      "source": [
        "# Pull out a single value from a 2-rank tensor\n",
        "print(rank_2_tensor[1, 1].numpy())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3RLCzAOHjfEH"
      },
      "source": [
        "We can combine integers and slices when indexing too. This can be useful if, for example, we want a subset of an image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTqNqsfJkJP_"
      },
      "outputs": [],
      "source": [
        "# Get row and column tensors\n",
        "print(\"Second row:\", rank_2_tensor[1, :].numpy())\n",
        "print(\"Second column:\", rank_2_tensor[:, 1].numpy())\n",
        "print(\"Last row:\", rank_2_tensor[-1, :].numpy())\n",
        "print(\"First item in last column:\", rank_2_tensor[0, -1].numpy())\n",
        "print(\"Skip the first row:\")\n",
        "print(rank_2_tensor[1:, :].numpy(), \"\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P45TwSUVSK6G"
      },
      "source": [
        "An example for a tensor with 3 axes:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GuLoMoCVSLxK"
      },
      "outputs": [],
      "source": [
        "print(rank_3_tensor[:, :, 4])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9NgmHq27TJOE"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "<th colspan=2>Selecting the last feature across all locations in each example in the batch </th>\n",
        "</tr>\n",
        "<tr>\n",
        "    <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/index1.png?raw=1\" alt=\"A 3x2x5 tensor with all the values at the index-4 of the last axis selected.\">\n",
        "  </td>\n",
        "      <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/index2.png?raw=1\" alt=\"The selected values packed into a 2-axis tensor.\">\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t9V83-thHn89"
      },
      "source": [
        "More on how to apply indexing can be found in the [tensor slicing guide](https://tensorflow.org/guide/tensor_slicing)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpr7R0t4SVb0"
      },
      "source": [
        "## Manipulating tensor shapes\n",
        "\n",
        "Sometimes, we need to reshape a tensor. For example, we flatten rank 2 tensors to rank 1 when computing things like confusion matrices.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nOcRxDC3jNIU"
      },
      "source": [
        "Notes:\n",
        "*   We can reshape a tensor to a new shape so long as it entails the same total number of elements (size).\n",
        "*   Shifting the order of axes is done with `tf.transpose` instead of `tf.reshape`.\n",
        "*   You may run across not-fully-specified shapes. Either the shape contains a `None` (an axis-length is unknown) or the whole shape is `None` (the rank of the tensor is unknown). This is often useful for machine learning when the images in a batch may have non-uniform width and height dimensions.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-SY3pTQ8bLyq"
      },
      "outputs": [],
      "source": [
        "# Shape returns a `TensorShape` object that shows the size along each axis\n",
        "x = tf.constant([[1, 2, 3],\n",
        "                [4, 5, 6],\n",
        "                [7, 8, 9]])\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "ZL-qTKj3bZ-z"
      },
      "outputs": [],
      "source": [
        "# Flatten the rank 2 tensor to rank 1 (a vector)\n",
        "x_flat = tf.reshape(x, [-1])\n",
        "print(x_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EMeTtga5Wq8j"
      },
      "outputs": [],
      "source": [
        "# Shape returns a `TensorShape` object that shows the size along each axis\n",
        "x = tf.constant([[1], [2], [3]])\n",
        "print(x.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "38jc2RXziT3W"
      },
      "outputs": [],
      "source": [
        "# You can convert this object into a Python list, too\n",
        "print(x.shape.as_list())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_xRlHZMKYnF"
      },
      "source": [
        "A tensor can be manipulated into a new shape using the `tf.reshape` operation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pa9JCgMLWy87"
      },
      "outputs": [],
      "source": [
        "# You can reshape a tensor to a new shape.\n",
        "# Note that you're passing in a list\n",
        "reshaped = tf.reshape(x, [1, 3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mcq7iXOkW3LK"
      },
      "outputs": [],
      "source": [
        "print(x.shape)\n",
        "print(reshaped.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gIB2tOkoVr6E"
      },
      "source": [
        "The original tensor and the newly created one are both held in memory (remember tensors are immutable). TensorFlow abides by C-style \"row-major\" memory ordering, which means that an increment on the rightmost index corresponds to a single step in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7kMfM0RpUgI8"
      },
      "outputs": [],
      "source": [
        "print(rank_3_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TcDtfQkJWzIx"
      },
      "source": [
        "In fact, flattening a tensor shows the order in which it is held in memory."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "COnHEPuaWDQp"
      },
      "outputs": [],
      "source": [
        "# A `-1` passed in the `shape` argument says \"Whatever fits\".\n",
        "print(tf.reshape(rank_3_tensor, [-1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jJZRira2W--c"
      },
      "source": [
        "Generally we use `tf.reshape` to combine or split adjacent axes (or add/remove `1`s, similar to `np.squeeze`).\n",
        "\n",
        "Taking the below 3x2x5 tensor, we can reshape to (3x2)x5 or 3x(2x5) for example."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zP2Iqc7zWu_J"
      },
      "outputs": [],
      "source": [
        "print(tf.reshape(rank_3_tensor, [3*2, 5]), \"\\n\")\n",
        "print(tf.reshape(rank_3_tensor, [3, -1]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6ZsZRUhihlDB"
      },
      "source": [
        "<table>\n",
        "<th colspan=3>\n",
        "Some good reshapes.\n",
        "</th>\n",
        "<tr>\n",
        "  <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/reshape-before.png?raw=1\" alt=\"A 3x2x5 tensor\">\n",
        "  </td>\n",
        "  <td>\n",
        "  <img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/reshape-good1.png?raw=1\" alt=\"The same data reshaped to (3x2)x5\">\n",
        "  </td>\n",
        "  <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/reshape-good2.png?raw=1\" alt=\"The same data reshaped to 3x(2x5)\">\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fDmFtFM7k0R2"
      },
      "source": [
        "## Tensor `DTypes`\n",
        "\n",
        "We can find a `tf.Tensor`'s data type by inspecting the `Tensor.dtype` property.\n",
        "\n",
        "Datatypes can be optionally specified when creating a `tf.Tensor` from a Python object. If left unspecified, TensorFlow inuits and assigns a datatype that can reasonably represent the data. By default, TensorFlow translates Python integers as `tf.int32` and Python floating point numbers as `tf.float32`.\n",
        "\n",
        "Tensors can be cast to other types."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5mSTDWbelUvu"
      },
      "outputs": [],
      "source": [
        "the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)\n",
        "the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)\n",
        "# Now, cast to an uint8 and lose the decimal precision\n",
        "the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)\n",
        "print(the_u8_tensor)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1yBlJsVlFSu"
      },
      "source": [
        "## Broadcasting\n",
        "\n",
        "Broadcasting in TensorFlow is based on the [equivalent method in NumPy](https://numpy.org/doc/stable/user/basics.broadcasting.html).  The premise is that sometimes we need to stretch smaller tensors automatically to fit the size of larger tensors to run combined operations on them.\n",
        "\n",
        "A simple example is multiplying or adding a tensor to a scalar. The scalar in this case is broadcast to the same shape as the tensor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P8sypqmagHQN"
      },
      "outputs": [],
      "source": [
        "x = tf.constant([1, 2, 3])\n",
        "\n",
        "y = tf.constant(2)\n",
        "z = tf.constant([2, 2, 2])\n",
        "# All of these are the same computation\n",
        "print(tf.multiply(x, 2))\n",
        "print(x * y)\n",
        "print(x * z)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o0SBoR6voWcb"
      },
      "source": [
        "Furthermore, tensors with an axis of length 1 can be stretched to match the axis length of another tensor during a combined operation.  \n",
        "\n",
        "As an example, a 3x1 matrix can be element-wise multiplied by a 1x4 matrix to result in a 3x4 matrix. The notation of the axis with length 1 is optional. In other words, the real shape of y is `[4]`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6sGmkPg3XANr"
      },
      "outputs": [],
      "source": [
        "# These are the same computations\n",
        "x = tf.reshape(x,[3,1])\n",
        "y = tf.range(1, 5)\n",
        "print(x, \"\\n\")\n",
        "print(y, \"\\n\")\n",
        "print(tf.multiply(x, y))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_7sh-EUYLrE"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>A broadcasted add: a <code>[3, 1]</code> times a <code>[1, 4]</code> gives a <code>[3,4]</code> </th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/broadcasting.png?raw=1\" alt=\"Adding a 3x1 matrix to a 4x1 matrix results in a 3x4 matrix\">\n",
        "  </td>\n",
        "</tr>\n",
        "</table>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9V3KgSJcKDRz"
      },
      "source": [
        "The same operation without broadcasting looks like this:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "elrF6v63igY8"
      },
      "outputs": [],
      "source": [
        "x_stretch = tf.constant([[1, 1, 1, 1],\n",
        "                         [2, 2, 2, 2],\n",
        "                         [3, 3, 3, 3]])\n",
        "\n",
        "y_stretch = tf.constant([[1, 2, 3, 4],\n",
        "                         [1, 2, 3, 4],\n",
        "                         [1, 2, 3, 4]])\n",
        "\n",
        "print(x_stretch * y_stretch)  # Again, operator overloading"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14KobqYu85gi"
      },
      "source": [
        "Broadcasting is usually both time and space efficient, as it never creates the expanded tensors in memory.  \n",
        "\n",
        "The broadcasting operation can be observed with `tf.broadcast_to`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GW2Q59_r8hZ6"
      },
      "outputs": [],
      "source": [
        "print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z2bAMMQY-jpP"
      },
      "source": [
        "More on broadcasting can be found in [this section](https://jakevdp.github.io/PythonDataScienceHandbook/02.05-computation-on-arrays-broadcasting.html) of Jake VanderPlas's book _Python Data Science Handbook_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o4Rpz0xAsKSI"
      },
      "source": [
        "## tf.convert_to_tensor\n",
        "\n",
        "As a reminder, most TensorFlow operations that expect arguments of class `tf.Tensor`, such as `tf.matmul` and `tf.reshape`, will also accept Python objects shaped like tensors.\n",
        "\n",
        "Under the hood, most TensorFlow ops apply `convert_to_tensor` to non-tensor arguments, such as NumPy's `ndarray`, `TensorShape`, Python lists, and `tf.Variable`, all of which will convert automatically.\n",
        "\n",
        "See the conversion registry `tf.register_tensor_conversion_function` to know whether a data type will automatically convert."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9njclVkkN7G"
      },
      "source": [
        "## String tensors\n",
        "\n",
        "`tf.string` is a `dtype` that represents data as strings (variable-length byte arrays) in tensors.\n",
        "\n",
        "However, string tensors are atomic meaning that they cannot be indexed the way Python strings can. See `tf.strings` for ways to manipulate them."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5P_8spEGQ0wp"
      },
      "source": [
        "An example of a scalar string tensor:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sBosmM8MkIh4"
      },
      "outputs": [],
      "source": [
        "# Tensors can be strings, too here is a scalar string.\n",
        "scalar_string_tensor = tf.constant(\"Lima Peru\")\n",
        "print(scalar_string_tensor)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "41Dv2kL9QrtO"
      },
      "outputs": [],
      "source": [
        "# If you have three string tensors of different lengths, this is OK.\n",
        "tensor_of_strings = tf.constant([\"Lima Peru\",\n",
        "                                 \"NASA SERVIR\",\n",
        "                                 \"Development Seed\"])\n",
        "# Note that the shape is (3,). The string length is not included.\n",
        "print(tensor_of_strings)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "76gQ9qrgSMzS"
      },
      "source": [
        "The `b` prefix in the above printout indicates that the `tf.string` dtype is a byte-string, not a unicode string. For more on how to use unicode text in TensorFlow see the [Unicode Tutorial](https://www.tensorflow.org/tutorials/load_data/unicode)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClSBPK-lZBQp"
      },
      "source": [
        "Also worth noting, unicode characters in TensorFlow are utf-8 encoded."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ir9cY42MMAei"
      },
      "source": [
        "`tf.strings` contains some basic string operations, including `tf.strings.split`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8k2K0VTFyj8e"
      },
      "outputs": [],
      "source": [
        "# You can use split to split a string into a set of tensors\n",
        "print(tf.strings.split(scalar_string_tensor, sep=\" \"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zgGAn1dfR-04"
      },
      "outputs": [],
      "source": [
        "# ...but it turns into a what is known as a `RaggedTensor` if you split up a tensor of strings,\n",
        "# as each string might be split into a different number of parts.\n",
        "print(tf.strings.split(tensor_of_strings))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "st9OxrUxWSKY"
      },
      "source": [
        "And we also have `tf.string.to_number`:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3nRtx3X9WRfN"
      },
      "outputs": [],
      "source": [
        "text = tf.constant(\"1 10 100\")\n",
        "print(tf.strings.to_number(tf.strings.split(text, \" \")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fE7nKJ2YW3aY"
      },
      "source": [
        "In TensorFlow, the `tf.string` dtype describes all raw bytes data. The `tf.io` module is used to convert data to and from bytes, as is done when decoding images and parsing text."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ua8BnAzxkRKV"
      },
      "source": [
        "## Sparse tensors\n",
        "\n",
        "Data can sometimes be sparse, as might be the case for a wide embedding space.  TensorFlow has a data type called `tf.sparse.SparseTensor` and related operations to support sparse data and work efficiently with it."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mS5zgqgUTPRb"
      },
      "source": [
        "<table>\n",
        "<tr>\n",
        "  <th>A `tf.SparseTensor`, shape: <code>[3, 4]</code></th>\n",
        "</tr>\n",
        "<tr>\n",
        "  <td>\n",
        "<img src=\"https://github.com/tensorflow/docs/blob/master/site/en/guide/images/tensor/sparse.png?raw=1\" alt=\"An 3x4 grid, with values in only two of the cells.\">\n",
        "  </td>\n",
        "</tr>\n",
        "</table>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "B9nbO1E2kSUN"
      },
      "outputs": [],
      "source": [
        "# Sparse tensors store values by index in a memory-efficient manner\n",
        "sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],\n",
        "                                       values=[1, 2],\n",
        "                                       dense_shape=[3, 4])\n",
        "print(sparse_tensor, \"\\n\")\n",
        "\n",
        "# You can convert sparse tensors to dense\n",
        "print(tf.sparse.to_dense(sparse_tensor))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FD7Ur2bkHxn8"
      },
      "source": [
        "# Functions\n",
        "\n",
        "In TensorFlow 2, [eager execution](eager.ipynb) mode is on by default. This mode enables intuitive use, rapid experimentation and flexibility. However, this mode faces limits with performance and deployability.\n",
        "\n",
        "`tf.function` is used to make graphs out of programs. The critical case for it is when we need to build Python-independent dataflow graphs from Python code, with goal of creating performant and portable models. In fact, compiling in `tf.function` is required to use the `SavedModel` class.\n",
        "\n",
        "We will discuss some of the main uses for `tf.function`.\n",
        "\n",
        "Before that, it's worth knowing how and when `tf.function` is recommended:\n",
        "\n",
        "1. When you need actually need low-level performance optimizations. Using tf.function to construct your model will assemble a Tensorflow graph, allowing you to execute your computation outside of eager mode more efficiently.\n",
        "2. When you need to port your model to different runtime environments, like mobile or edge devices.\n",
        "\n",
        "Debugging is best done in eager mode, then functions from that should be decorated with `@tf.function`. It's important to note that a `tf.function` avoids Python side effects like object mutation or list appends. For example, if you append to a list as a side effect, the append operation will only happen once, when the graph is traced, not every time the graph function is called.\n",
        "\n",
        "This can be a useful feature if you want to ensure that your function doesn't have unintended side effects. However, it can also be a source of confusion if you're expecting Python-style side effects. The best practice is to avoid relying on these Python side effects when using tf.functions, convert python iterables to Tensorflow Datasets, and return any values you need from the function.\n",
        "\n",
        "Some notes on Keras w.r.t. `tf.function`:\n",
        "\n",
        "For machine learning, if you don't need low-level control of your model training, use the Keras API.\n",
        "- The Keras API accepts NumPy arrays, Python generators and, `tf.data.Dataset`s.\n",
        "- It abstracts away the implementation of regularization, activation, and loss calculation, all of which are easily included.\n",
        "- The Keras API supports `tf.distribute` to increase the compute efficiency regardless of the hardware configuration.\n",
        "- It supports arbitrary/custom losses and metrics.\n",
        "- The Keras API supports callbacks such as `tf.keras.callbacks.TensorBoard` for enhanced logging and visualization, and also custom callbacks.\n",
        "- It is easy to use and performant, as it automatically uses TensorFlow graphs under the hood.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GEkEScgH9Bd"
      },
      "source": [
        "### Usage\n",
        "\n",
        "A `Function` in TensorFlow, (which can be created by adding the `@tf.function` decorator) behaves like a core TensorFlow operation. It can be executed eagerly."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rNy_cPdAH9pY"
      },
      "outputs": [],
      "source": [
        "@tf.function  # The decorator converts `add` into a `Function`.\n",
        "def add(a, b):\n",
        "  return a + b\n",
        "\n",
        "add(tf.ones([2, 2]), tf.ones([2, 2]))  #  [[2., 2.], [2., 2.]]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l151qACaIENW"
      },
      "source": [
        "`Function`s can be used inside other `Function`s."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NUYHnGfgIACj"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def dense_layer(x, w, b):\n",
        "  return add(tf.matmul(x, w), b)\n",
        "\n",
        "dense_layer(tf.ones([3, 2]), tf.ones([2, 2]), tf.ones([2]))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xtJfg4hmIK3q"
      },
      "source": [
        "In some cases, `Function`s are faster than their eager code equivalent. This is especially so for graphs with lots of small ops. However, on the flip side graphs with few expensive ops (like convolutions) may not benefit from much speedup."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PIdZjMj-ILYN"
      },
      "outputs": [],
      "source": [
        "import timeit\n",
        "conv_layer = tf.keras.layers.Conv2D(100, 3)\n",
        "\n",
        "@tf.function\n",
        "def conv_fn(image):\n",
        "  return conv_layer(image)\n",
        "\n",
        "image = tf.zeros([1, 200, 200, 100])\n",
        "# Warm up\n",
        "conv_layer(image); conv_fn(image)\n",
        "print(\"Eager conv:\", timeit.timeit(lambda: conv_layer(image), number=10))\n",
        "print(\"Function conv:\", timeit.timeit(lambda: conv_fn(image), number=10))\n",
        "print(\"Note how there's not much difference in performance for convolutions\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyD8-Jr0IXH2"
      },
      "source": [
        "#### Use unknown dimensions for flexibility\n",
        "\n",
        "  TensorFlow matches tensors on the basis of their shape. We can use a `None` dimension as a wildcard to allow `Function`s to run on variably-sized input. Variably-sized input is frequent when dealing with sequences of different length, or images of different spatial dimensions (sizes) across batches."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mHKNuNj6IXvL"
      },
      "outputs": [],
      "source": [
        "@tf.function(input_signature=(tf.TensorSpec(shape=[None], dtype=tf.int32),))  # Note this still works without the arguments\n",
        "def g(x):\n",
        "  print(x)\n",
        "  return x\n",
        "\n",
        "print(g(tf.constant([1, 2, 3])))\n",
        "print(g(tf.constant([1, 2, 3, 4, 5])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0m3v0vHZIptO"
      },
      "source": [
        "#### Using iterators and generators\n",
        "\n",
        "TensorFlow has a `tf.data.Iterator` for iteration. The [`tf.data`](https://www.tensorflow.org/guide/data) API helps implement generator patterns. TensorFlow's iterators and generators help avoid Python side effects which can surface outside of eager mode."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "v6J3wre3Is_M"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def buggy_consume_next(iterator):\n",
        "  tf.print(\"Value:\", next(iterator))\n",
        "\n",
        "iterator = iter([1, 2, 3])\n",
        "buggy_consume_next(iterator)\n",
        "# This reuses the first value from the iterator, rather than consuming the next value.\n",
        "buggy_consume_next(iterator)\n",
        "buggy_consume_next(iterator)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LSLnGbzyIw2s"
      },
      "outputs": [],
      "source": [
        "@tf.function\n",
        "def good_consume_next(iterator):\n",
        "  # This is ok, iterator is a tf.data.Iterator\n",
        "  tf.print(\"Value:\", next(iterator))\n",
        "\n",
        "ds = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
        "iterator = iter(ds)\n",
        "good_consume_next(iterator)\n",
        "good_consume_next(iterator)\n",
        "good_consume_next(iterator)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JQrtvU81ySYp"
      },
      "source": [
        "# Data\n",
        "\n",
        "The `tf.data.Dataset` [API](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) represents methods to create tensor-formatted datasets out of many different types of data.\n",
        "\n",
        "Usage of the API follows the below pattern:\n",
        "\n",
        "*   Create a dataset from your input data.\n",
        "*   Iterate over the dataset, applying any dataset transformations to the data.\n",
        "\n",
        "Note that iteration occurs through streaming, so it doesn't matter if a full dataset can't fit into memory.\n",
        "\n",
        "The simplest example of creating a dataset is from a python list:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rLqqLeIPywjo"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
        "for element in dataset:\n",
        "  print(element)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yXhDZA5gzKHq"
      },
      "source": [
        "Transformations:\n",
        "With a dataset, you can use transformations to prepare the data. In machine learning, sometimes we want to augment our data with image flips and rotations for example. Those could be done in the pipeline too."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ymCUQjZezKzW"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices([1, 2, 3])\n",
        "dataset = dataset.map(lambda x: x*2)\n",
        "list(dataset.as_numpy_iterator())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rqoxr6q5zUk5"
      },
      "source": [
        "Common Terms:\n",
        "\n",
        "\n",
        "*   **Element**: A single result from calling next() on a dataset iterator. Elements are sometimes nested structures containing multiple components.\n",
        "\n",
        "*   **Component**: The single part in a nested structure of an element.\n",
        "\n",
        "Supported types:\n",
        "\n",
        "As nested structures, elements can contain tuples, named tuples, and dictionaries. This is different behavior than that of Python lists. Instead, Python lists require conversion to tensors and then may be treated as components. Element components can express as any type represented in `tf.TypeSpec`, which includes `tf.Tensor`, `tf.data.Datase`t, `tf.sparse.SparseTensor`, and `tf.TensorArray`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMZQ62iEztLl"
      },
      "outputs": [],
      "source": [
        "a = 1 # Integer element\n",
        "b = 2.0 # Float element\n",
        "c = (1, 2) # Tuple element with 2 components\n",
        "d = {\"a\": (2, 2), \"b\": 3} # Dict element with 3 components\n",
        "Point = collections.namedtuple(\"Point\", [\"x\", \"y\"])\n",
        "e = Point(1, 2) # Named tuple\n",
        "f = tf.data.Dataset.range(10) # Dataset element"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PMyoA9dDDUoe"
      },
      "source": [
        "We can directly read images and labels from `np.array` format into `tf.data.dataset`. In the below example, we are creating synthetic 3 channel images and single channel labels with uniform spatial dimensions and subsequently reading them into a dataset using the `tf.data.Dataset.from_tensor_slices` method. This works if you can fit the NumPy arrays in memory for the images and labels, but if not you'll need to use a data pipe (example in later tutorial)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "5KTmGzvhB49E"
      },
      "outputs": [],
      "source": [
        "image1, image2, image3 = np.zeros((3, 10,10)), np.zeros((3, 10,10)), np.zeros((3, 10,10))\n",
        "label1, label2, label3 = np.ones((10,10)), np.ones((10,10)), np.ones((10,10))\n",
        "\n",
        "images = [image1, image2, image3]\n",
        "labels = [label1, label2, label3]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FXOA6qJ5DHYN"
      },
      "outputs": [],
      "source": [
        "dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dK4_ZE2K9ziw"
      },
      "source": [
        "### Data pipelines\n",
        "\n",
        "We can use the `tf.data.Dataset` API iterators and generators to construct data pipelines. A data pipeline is used to map a set of processing steps on a large amount of data. With such pipelines, we can stream data iteratively to the same reusable code for processing and have it contribute to the same dataset (e.g. training or validation). As an example, an image data pipeline might involve reading image files from a file system, applying random perturbations or other augmentations to each image, and then randomly shuffling and appending images to a batch for training. In essence, data pipelines built with the `tf.data` API enable large amounts of data to be read, with support for different data formats, and complex transformations. We will explore a data pipeline in a subsequent lesson, but if you're curious about data pipelines, you can read more via this [guide](https://www.tensorflow.org/guide/data).\n",
        "\n",
        ":::{figure-md} datapipe-fig\n",
        "<img src=\"https://storage.googleapis.com/jalammar-ml/tf.data/images/tf.data-simple-pipeline.png\" width=\"650px\">\n",
        "\n",
        "Figure from [Intro to Data Input Pipelines with tf.data\n",
        "](https://www.kaggle.com/code/jalammar/intro-to-data-input-pipelines-with-tf-data)\n",
        ":::\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XQKT5K5s8cxe"
      },
      "source": [
        "This discussion on datasets is different than the [TensorFlow Datasets collection](https://www.tensorflow.org/datasets) which hosts prepared datasets for TensorFlow models and analysis. We will explore use of the TensorFlow Datasets collection also in other lessons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#@title Licensed under the Apache License, Version 2.0 (the \"License\");\n",
        "# you may not use this file except in compliance with the License.\n",
        "# You may obtain a copy of the License at\n",
        "#\n",
        "# https://www.apache.org/licenses/LICENSE-2.0\n",
        "#\n",
        "# Unless required by applicable law or agreed to in writing, software\n",
        "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
        "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
        "# See the License for the specific language governing permissions and\n",
        "# limitations under the License."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
