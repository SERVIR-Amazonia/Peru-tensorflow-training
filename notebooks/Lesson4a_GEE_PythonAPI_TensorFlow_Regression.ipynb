{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_SHAc5qbiR8l"
      },
      "source": [
        "# Regression using TensorFlow with Google Earth Engine Python API\n",
        "\n",
        "This tutorial demonstrates how to train a model to predict continuous output (regression) from geospatial data. To do this, we will use the impervious surface area dataset from [NLCD](https://www.mrlc.gov/data) and a Landsat 8 composite. Both datasets will be acquired from Google Earth Engine. Using the Keras sequential API, we will show how to train a [U-Net](https://arxiv.org/abs/1505.04597) model to predict impervious surface area percentage for each pixel in an image. \n",
        "\n",
        ":::{figure-md} imperviousfig\n",
        "<img src=\"images/impervious_surface_cover_regression.png\" width=\"650px\">\n",
        ":::\n",
        "\n",
        "This tutorial is an adaptation of this [example](https://developers.google.com/earth-engine/guides/tf_examples#regression-with-an-fcnn).\n",
        "\n",
        "This tutorial covers:\n",
        "\n",
        "1.   Generating and exporting training/testing patches from Google Earth Engine (henceforth referred to as Earth Engine or EE).\n",
        "2.   Using an iterator to supply batches furing training and testing.\n",
        "3.   Training the regression model and saving it.\n",
        "4.   Generating predictions with the trained model and plotting them.\n",
        "\n",
        "## What is regression?\n",
        "Suppose we want to predict something like earthquake intensity or soil mositure given a satellite image. These express as continuous targets across pixels. This makes the objective different than classification, where a discrete set of classes are predicted across pixels. Models designed to predict a continuous output are performing regression. In a supervised context, the goal of regression is to produce results with the minimum difference (error) between the true and predicted values. We can approach this through a number of machine learning algorithms, to include deep learning with convolutional neural networks. You may have seen ConvNets used extensively for classification, but they have significant power for modeling regression problems. That said, it's important to be aware of when deep learning is useful for regression.\n",
        "\n",
        "There are two common types of regression problems:\n",
        "1. Linear, in which the input (independent) values are linearly correlated with the output (dependent) values. For example, if an input variable increases so does the output. Linear regression produces real values of an arbitrary range.\n",
        "2. Logistic, which does not assume a linear correlation between the input and output, but rather seeks to model liklihood (probabilities). Logistic regression produces propabilities so within the range of [0,1].\n",
        "\n",
        "\n",
        "Where a linear relationship exists, we don't need a complex approach to accurately predict the target dependent variable. It is problems in which the relationship between the independent variables is varied and context-dependent that we might use deep learning for regression. One of the intrinsic characteristics of deep learning is the non-linearity between the layers of a network, which makes them a useful tool for solving logistic regression type problems.\n",
        "\n",
        "The main distinctions that present when using ConvNets for regression instead of classification are:\n",
        "1. The labels are continuous. They will likely appear as float values between an arbitrary range that starts with zero. Note: It's often helpful in deep learning to scale raw values to a sensible range, e.g. [0,1].\n",
        "2. The loss function is different. A common loss function used in regression is mean squared error (MSE), which measures the average of the squared differences between the predicted and true values. The lower the value of MSE the better, with a perfect value being 0.0. Note: squaring is used to amplify and penalize larger differences more than smaller ones.\n",
        "3. For regression of real values, either a linear or no activation function is used for the output layer. For regression of probabilities, a logistic activation function is used. \n",
        "\n",
        ":::{figure-md} U-Net for regression\n",
        "<img src=\"https://www.researchgate.net/profile/Fenglin-Sun-2/publication/352181228/figure/fig2/AS:1032041529368581@1623069283910/A-U-net-regression-architecture-example-of-50-80-pixels-in-the-lowest-resolution.png\" width=\"450px\">\n",
        "\n",
        "[U-Net for regression](https://www.researchgate.net/profile/Fenglin-Sun-2/publication/352181228/figure/fig2/AS:1032041529368581@1623069283910/A-U-net-regression-architecture-example-of-50-80-pixels-in-the-lowest-resolution.png) from [\"Deep Learning-Based Radar Composite Reflectivity Factor Estimations from Fengyun-4A Geostationary Satellite Observations\" by Sun et. al, 2021](https://www.researchgate.net/publication/352181228_Deep_Learning-Based_Radar_Composite_Reflectivity_Factor_Estimations_from_Fengyun-4A_Geostationary_Satellite_Observations)\n",
        ":::\n",
        "\n",
        "In this tutorial, we'll look at a practical example of using satellite imagery with descriptive features to predict pixel-wise percentages. Naturally, our labels and predictions will be constrained to a range of [0,1]. The goals will be to train a model that:\n",
        "1. Generates output images in which pixels values are percentages learned from input images with descriptive features.\n",
        "2. Minimize the difference between the true percentages in the corresponding label images and the predicted percentages in the output images."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MJ4kW1pEhwP"
      },
      "source": [
        "## Imports and authentication"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "neIa46CpciXq"
      },
      "outputs": [],
      "source": [
        "# Cloud authentication.\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jat01FEoUMqg"
      },
      "outputs": [],
      "source": [
        "# Import, authenticate and initialize the Earth Engine library.\n",
        "import ee\n",
        "ee.Authenticate()\n",
        "ee.Initialize()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "V6lce1eu0C2E"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "#print(tf.__version__)\n",
        "from tensorflow.keras import layers, losses, models, metrics, optimizers\n",
        "\n",
        "import os, folium, glob, json, tifffile\n",
        "from PIL import Image\n",
        "import numpy as np\n",
        "from random import randrange\n",
        "import time\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.pyplot import imshow\n",
        "from pprint import pprint\n",
        "from google.colab import drive"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Some of the operations we'll be running, particularly model training, take prohibitively long when using Google Colab because the storage backend, Google Drive, has slow IO. \n",
        "\n",
        "So in some spots we'll use precomputed files from the `tf-eo-devseed-2-processed-outputs` Google Drive folder, which you can create a shortcut to by navigating to this link (no need to do this twice if you did it in an earlier lesson): https://drive.google.com/drive/folders/1FrSTn9Iq458qQhTw_7rbaVSc739TTw9V?usp=share_link\n",
        "\n",
        "We'll also create a directory in our personal Google Drive to store user generated outputs, `tf-eo-devseed-2-user_outputs_dir`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMt-vVkqRCM3"
      },
      "outputs": [],
      "source": [
        "if 'google.colab' in str(get_ipython()):\n",
        "    # mount google drive\n",
        "    drive.mount('/content/gdrive')\n",
        "    processed_outputs_dir = '/content/gdrive/My Drive/tf-eo-devseed-2-processed-outputs/'\n",
        "    user_outputs_dir = '/content/gdrive/My Drive/tf-eo-devseed-2-user_outputs_dir'\n",
        "    if not os.path.exists(user_outputs_dir):\n",
        "        os.makedirs(user_outputs_dir)\n",
        "    print('Running on Colab')\n",
        "else:\n",
        "    processed_outputs_dir = os.path.abspath(\"./data/tf-eo-devseed-2-processed-outputs\")\n",
        "    user_outputs_dir = os.path.abspath('./tf-eo-devseed-2-user_outputs_dir')\n",
        "    if not os.path.exists(user_outputs_dir):\n",
        "        os.makedirs(user_outputs_dir)\n",
        "        os.makedirs(processed_outputs_dir)\n",
        "    print(f'Not running on Colab, data needs to be downloaded locally at {os.path.abspath(processed_outputs_dir)}')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hFyH89P4RFbj"
      },
      "outputs": [],
      "source": [
        "# Move to your user_outputs_dir directory in order to write data\n",
        "%cd $user_outputs_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmfKLl9XcnGJ"
      },
      "source": [
        "## Set global parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Below we'll set up the `FEATURES_DICT` metadata objects that will help us access the satellite imagery bands we will use for training. We'll also define the paths to our dataset splits we'll use for training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psz7wJKalaoj"
      },
      "outputs": [],
      "source": [
        "# Specify names locations for outputs in Google Drive.\n",
        "FOLDER = 'fcnn-demo'\n",
        "PREDICTIONS = 'predictions'\n",
        "TEST_PATCHES = 'test_patches'\n",
        "TRAINING_BASE = 'training_patches'\n",
        "VAL_BASE = 'val_patches'\n",
        "\n",
        "# Specify inputs (Landsat bands) to the model and the response variable.\n",
        "opticalBands = ['B1', 'B2', 'B3', 'B4', 'B5', 'B6', 'B7']\n",
        "thermalBands = ['B10', 'B11']\n",
        "BANDS = opticalBands + thermalBands\n",
        "RESPONSE = 'impervious'\n",
        "FEATURES = BANDS + [RESPONSE]\n",
        "\n",
        "# Specify the size and shape of patches expected by the model.\n",
        "KERNEL_SIZE = 256\n",
        "KERNEL_SHAPE = [KERNEL_SIZE, KERNEL_SIZE]\n",
        "COLUMNS = [\n",
        "  tf.io.FixedLenFeature(shape=KERNEL_SHAPE, dtype=tf.float32) for k in FEATURES\n",
        "]\n",
        "FEATURES_DICT = dict(zip(FEATURES, COLUMNS))\n",
        "FEATURES_DICT"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Then, we'll specify some training parameters. \n",
        "\n",
        "Since this is a regression problem, we'll use Mean Squared Error, as opposed to another metric for classification (Accuracy) or segmentation (Dice/Intsersection over Union).\n",
        "\n",
        "Larger batch sizes are better, as they allow the model to\n",
        "1. more accurately estimate the gradient of the loss function\n",
        "2. more fully utilize GPU resources for efficient training\n",
        "\n",
        "Batch size can also affect model generalization, but it is somewhat configuration and problem dependent on whether this helps or hurts model generalization. It's best to experiment to find out! See [Revisiting Small Batch Training for Deep Neural Networks](https://arxiv.org/abs/1804.07612) for an investigation on this issue.\n",
        "\n",
        "SGD is a standard optimizer for deep learning models."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Sizes of the training and evaluation datasets.\n",
        "TRAIN_SIZE = 16000\n",
        "VAL_SIZE = 8000\n",
        "\n",
        "# Specify model training parameters.\n",
        "BATCH_SIZE = 16\n",
        "EPOCHS = 10\n",
        "BUFFER_SIZE = 2000\n",
        "OPTIMIZER = 'SGD'\n",
        "LOSS = 'MeanSquaredError'\n",
        "METRICS = ['RootMeanSquaredError']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WIoL0f3GXfNl"
      },
      "outputs": [],
      "source": [
        "dirs = [f\"{FOLDER}\", f\"{FOLDER}/{PREDICTIONS}\",  f\"{FOLDER}/{TEST_PATCHES}\"]\n",
        "for d in dirs:\n",
        "  if not os.path.exists(d):\n",
        "    os.makedirs(d)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IO52qQlUUc-D"
      },
      "outputs": [],
      "source": [
        "!ls fcnn-demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hgoDc7Hilfc4"
      },
      "source": [
        "# Imagery\n",
        "\n",
        "Collect and process the input imagery (cloud masking, compositing).  Display the composite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-IlgXu-vcUEY"
      },
      "outputs": [],
      "source": [
        "# Use Landsat 8 surface reflectance data.\n",
        "l8sr = ee.ImageCollection('LANDSAT/LC08/C01/T1_SR')\n",
        "\n",
        "# Cloud masking function.\n",
        "def maskL8sr(image):\n",
        "  cloudShadowBitMask = ee.Number(2).pow(3).int()\n",
        "  cloudsBitMask = ee.Number(2).pow(5).int()\n",
        "  qa = image.select('pixel_qa')\n",
        "  mask1 = qa.bitwiseAnd(cloudShadowBitMask).eq(0).And(\n",
        "    qa.bitwiseAnd(cloudsBitMask).eq(0))\n",
        "  mask2 = image.mask().reduce('min')\n",
        "  mask3 = image.select(opticalBands).gt(0).And(\n",
        "          image.select(opticalBands).lt(10000)).reduce('min')\n",
        "  mask = mask1.And(mask2).And(mask3)\n",
        "  return image.select(opticalBands).divide(10000).addBands(\n",
        "          image.select(thermalBands).divide(10).clamp(273.15, 373.15)\n",
        "            .subtract(273.15).divide(100)).updateMask(mask)\n",
        "\n",
        "# The image input data is a cloud-masked median composite.\n",
        "image = l8sr.filterDate('2015-01-01', '2017-12-31').map(maskL8sr).median()\n",
        "\n",
        "# Use folium to visualize the imagery.\n",
        "mapid = image.getMapId({'bands': ['B4', 'B3', 'B2'], 'min': 0, 'max': 0.3})\n",
        "map = folium.Map(location=[38., -122.5])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='median composite',\n",
        "  ).add_to(map)\n",
        "\n",
        "mapid = image.getMapId({'bands': ['B10'], 'min': 0, 'max': 0.5})\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='thermal',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gHznnctkJsZJ"
      },
      "source": [
        "Collect the labels (impervious surface area (in fraction of a pixel)) and display."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e0wHDyxVirec"
      },
      "outputs": [],
      "source": [
        "nlcd = ee.Image('USGS/NLCD/NLCD2016').select('impervious')\n",
        "nlcd = nlcd.divide(100).float()\n",
        "\n",
        "mapid = nlcd.getMapId({'min': 0, 'max': 1})\n",
        "map = folium.Map(location=[38., -122.5])\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='nlcd impervious',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CTS7_ZzPDhhg"
      },
      "source": [
        "Now we will combine the Landsat composite and NLCD impervious surface raster into a single stacked image array. From that, we will break the image down into patches with width and height of 256 pixels.\n",
        "\n",
        "To convert the EE multi-band image collection to an image array, we use [`neighborhoodToArray()`](https://developers.google.com/earth-engine/api_docs#eeimageneighborhoodtoarray), then proceed to sample the image at selective areas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eGHYsdAOipa4"
      },
      "outputs": [],
      "source": [
        "featureStack = ee.Image.cat([\n",
        "  image.select(BANDS),\n",
        "  nlcd.select(RESPONSE)\n",
        "]).float()\n",
        "\n",
        "list = ee.List.repeat(1, KERNEL_SIZE)\n",
        "lists = ee.List.repeat(list, KERNEL_SIZE)\n",
        "kernel = ee.Kernel.fixed(KERNEL_SIZE, KERNEL_SIZE, lists)\n",
        "\n",
        "arrays = featureStack.neighborhoodToArray(kernel)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F4djSxBRG2el"
      },
      "source": [
        "Use some pre-made geometries to sample the stack in strategic locations.  Specifically, these are hand-made polygons in which to take the 256x256 samples.  Display the sampling polygons on a map, red for training polygons, blue for evaluation.\n",
        "\n",
        "We will strategiclly sample the imagery using some diverse, representative geometries. The red geometries plotted below are for training, while the blue are for validation."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ure_WaD0itQY"
      },
      "outputs": [],
      "source": [
        "trainingPolys = ee.FeatureCollection('projects/google/DemoTrainingGeometries')\n",
        "valPolys = ee.FeatureCollection('projects/google/DemoEvalGeometries')\n",
        "\n",
        "polyImage = ee.Image(0).byte().paint(trainingPolys, 1).paint(valPolys, 2)\n",
        "polyImage = polyImage.updateMask(polyImage)\n",
        "\n",
        "mapid = polyImage.getMapId({'min': 1, 'max': 2, 'palette': ['red', 'blue']})\n",
        "map = folium.Map(location=[38., -100.], zoom_start=5)\n",
        "folium.TileLayer(\n",
        "    tiles=mapid['tile_fetcher'].url_format,\n",
        "    attr='Map Data &copy; <a href=\"https://earthengine.google.com/\">Google Earth Engine</a>',\n",
        "    overlay=True,\n",
        "    name='training polygons',\n",
        "  ).add_to(map)\n",
        "map.add_child(folium.LayerControl())\n",
        "map"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AkzjoOwSJrft"
      },
      "outputs": [],
      "source": [
        "# How many polygons do we have in total?\n",
        "trainingPolys.size().getInfo(), valPolys.size().getInfo()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZV890gPHeZqz"
      },
      "source": [
        "# Sampling\n",
        "\n",
        "Now we will use those geometries to extract samples from the stacked image array. Within each geometry we take a 256x256 neighborhood of pixels around several points, shard them to prevent memory error and then collect them into a single tfrecord. This is done for the training and validation data separately.\n",
        "\n",
        "Note: for brevity's sake in this tutorial, we are only sampling from 2 training geometries. You can revise `range(2): #range(trainingPolys.size().getInfo())` if you wish to use all geometries."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "both",
        "id": "FyRpvwENxE-A"
      },
      "outputs": [],
      "source": [
        " # Convert the feature collections to lists for iteration.\n",
        "trainingPolysList = trainingPolys.toList(trainingPolys.size())\n",
        "valPolysList = valPolys.toList(valPolys.size())\n",
        "\n",
        "# These numbers determined experimentally.\n",
        "n = 200 # Number of shards in each polygon.\n",
        "N = 2000 # Total sample size in each polygon.\n",
        "\n",
        "# Export all the training data (in many pieces), with one task\n",
        "# per geometry.\n",
        "for g in range(2): #range(trainingPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(trainingPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n, # Size of the shard.\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "    #print(geomSample)\n",
        "\n",
        "  desc = TRAINING_BASE + '_g' + str(g)\n",
        "  task_train = ee.batch.Export.table.toDrive(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    folder = FOLDER,\n",
        "    fileNamePrefix = desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  print(FOLDER, desc)\n",
        "  task_train.start()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We'll do a similar sampling procedure for the validation data. To keep the dataset size reasonable for the Colab + Google Drive setup, we will only sample and export 1 validation polygon. Revise the corresponding line for the validation set to sample from the full range of validation polygons."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# Export all the evaluation data.\n",
        "for g in range(1): #range(valPolys.size().getInfo()):\n",
        "  geomSample = ee.FeatureCollection([])\n",
        "  for i in range(n):\n",
        "    sample = arrays.sample(\n",
        "      region = ee.Feature(valPolysList.get(g)).geometry(),\n",
        "      scale = 30,\n",
        "      numPixels = N / n,\n",
        "      seed = i,\n",
        "      tileScale = 8\n",
        "    )\n",
        "    geomSample = geomSample.merge(sample)\n",
        "\n",
        "  desc = VAL_BASE + '_g' + str(g)\n",
        "  task_validation = ee.batch.Export.table.toDrive(\n",
        "    collection = geomSample,\n",
        "    description = desc,\n",
        "    folder = FOLDER,\n",
        "    fileNamePrefix = desc,\n",
        "    fileFormat = 'TFRecord',\n",
        "    selectors = BANDS + [RESPONSE]\n",
        "  )\n",
        "  print(FOLDER, desc)\n",
        "  task_validation.start()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "These tasks take a while depending on the amount of data. In this case, we're exporting a small TFRecord but it still takes 20 minutes because of slow IO to Google Drive. We can use the EE API's `getTaskStatus` function to report the status of the background export task."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def check_task_status(task):\n",
        "    while True:\n",
        "        status = ee.data.getTaskStatus(task.id)\n",
        "        if not status:  # if the status list is empty\n",
        "            print(\"Task not found.\")\n",
        "            break\n",
        "\n",
        "        state = status[0].get('state')\n",
        "        if state == 'RUNNING':\n",
        "            print(\"Task is still running.\")\n",
        "            time.sleep(10)  # waits for n seconds before checking again\n",
        "        else:\n",
        "            print(f\"Task is no longer running. Current state: {state}\")\n",
        "            break\n",
        "\n",
        "check_task_status(task_validation)\n",
        "check_task_status(task_train)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Once the tasks finish, we can see the results here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!ls fcnn-demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Since the above cell takes a long time to run, let's cancel it and switch to using the directory with the prepared intermediate and final data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd $processed_outputs_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "and check that the files are there"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "!ls fcnn-demo"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rWXrvBE4607G"
      },
      "source": [
        "# Training data\n",
        "\n",
        "Now that we have exported a TFRecord from Earth Engine, let's load it into a `tf.data.Dataset`.  Unfortunately, there isn't a path to load the data directly from Earth Engine into a `tf.data.Dataset`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WWZ0UXCVMyJP"
      },
      "outputs": [],
      "source": [
        "def parse_tfrecord(example_proto):\n",
        "  \"\"\"The parsing function.\n",
        "  Read a serialized example into the structure defined by FEATURES_DICT.\n",
        "  Args:\n",
        "    example_proto: a serialized Example.\n",
        "  Returns:\n",
        "    A dictionary of tensors, keyed by feature name.\n",
        "  \"\"\"\n",
        "  return tf.io.parse_single_example(example_proto, FEATURES_DICT)\n",
        "\n",
        "\n",
        "def to_tuple(inputs):\n",
        "  \"\"\"Function to convert a dictionary of tensors to a tuple of (inputs, outputs).\n",
        "  Turn the tensors returned by parse_tfrecord into a stack in HWC shape.\n",
        "  Args:\n",
        "    inputs: A dictionary of tensors, keyed by feature name.\n",
        "  Returns:\n",
        "    A tuple of (inputs, outputs).\n",
        "  \"\"\"\n",
        "  inputsList = [inputs.get(key) for key in FEATURES]\n",
        "  stacked = tf.stack(inputsList, axis=0)\n",
        "  # Convert from CHW to HWC\n",
        "  stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "  return stacked[:,:,:len(BANDS)], stacked[:,:,len(BANDS):]\n",
        "\n",
        "\n",
        "def get_dataset(pattern):\n",
        "  \"\"\"Function to read, parse and format to tuple a set of input tfrecord files.\n",
        "  Get all the files matching the pattern, parse and convert to tuple.\n",
        "  Args:\n",
        "    pattern: A file pattern to match in a google drive bucket.\n",
        "  Returns:\n",
        "    A tf.data.Dataset\n",
        "  \"\"\"\n",
        "  glob = tf.io.gfile.glob(pattern)\n",
        "  dataset = tf.data.TFRecordDataset(glob, compression_type='GZIP')\n",
        "  dataset = dataset.map(parse_tfrecord, num_parallel_calls=5)\n",
        "  dataset = dataset.map(to_tuple, num_parallel_calls=5)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xg1fa18336D2"
      },
      "source": [
        "Parse the training dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rm0qRF0fAYcC"
      },
      "outputs": [],
      "source": [
        "def get_training_dataset():\n",
        "\t\"\"\"Get the preprocessed training dataset\n",
        "  Returns:\n",
        "    A tf.data.Dataset of training data.\n",
        "  \"\"\"\n",
        "\tglobb = f\"{FOLDER}/{TRAINING_BASE}*\"\n",
        "\tdataset = get_dataset(globb)\n",
        "\tdataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "training = get_training_dataset()\n",
        "\n",
        "#view the first element\n",
        "#print(iter(training.take(1)).next())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j-cQO5RL6vob"
      },
      "source": [
        "# Validation data\n",
        "\n",
        "Parse the validation dataset.  Note that the validation dataset has a batch size of 1 which is different from the training dataset. Another distinction is that the validation dataset is not shuffled."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ieKTCGiJ6xzo"
      },
      "outputs": [],
      "source": [
        "def get_val_dataset():\n",
        "\t\"\"\"Get the preprocessed validation dataset\n",
        "  Returns:\n",
        "    A tf.data.Dataset of validation data.\n",
        "  \"\"\"\n",
        "\tglobb = f\"{FOLDER}/{VAL_BASE}*\"\n",
        "\tdataset = get_dataset(globb)\n",
        "\tdataset = dataset.batch(1).repeat()\n",
        "\treturn dataset\n",
        "\n",
        "validation = get_val_dataset()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JIE7Yl87lgU"
      },
      "source": [
        "# Model\n",
        "\n",
        "For our model, we will use a Keras implementation of the U-Net model, and provide the network with 256x256 pixel image patches as input. The output will be probabilities for each pixel (a continuous output).\n",
        "\n",
        "As this is a regression problem, we apply mean squared error as our loss function. As well, we implement a saturating activation function to address any artifacts that are produced from squashing the range of values to [0,1] (something we need to do to make a sensible measurement of impervious surface fraction per pixel)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wsnnnz56yS3l"
      },
      "outputs": [],
      "source": [
        "def conv_block(input_tensor, num_filters):\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(input_tensor)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\tencoder = layers.Conv2D(num_filters, (3, 3), padding='same')(encoder)\n",
        "\tencoder = layers.BatchNormalization()(encoder)\n",
        "\tencoder = layers.Activation('relu')(encoder)\n",
        "\treturn encoder\n",
        "\n",
        "def encoder_block(input_tensor, num_filters):\n",
        "\tencoder = conv_block(input_tensor, num_filters)\n",
        "\tencoder_pool = layers.MaxPooling2D((2, 2), strides=(2, 2))(encoder)\n",
        "\treturn encoder_pool, encoder\n",
        "\n",
        "def decoder_block(input_tensor, concat_tensor, num_filters):\n",
        "\tdecoder = layers.Conv2DTranspose(num_filters, (2, 2), strides=(2, 2), padding='same')(input_tensor)\n",
        "\tdecoder = layers.concatenate([concat_tensor, decoder], axis=-1)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\tdecoder = layers.Conv2D(num_filters, (3, 3), padding='same')(decoder)\n",
        "\tdecoder = layers.BatchNormalization()(decoder)\n",
        "\tdecoder = layers.Activation('relu')(decoder)\n",
        "\treturn decoder\n",
        "\n",
        "def get_model():\n",
        "\tinputs = layers.Input(shape=[None, None, len(BANDS)]) # 256\n",
        "\tencoder0_pool, encoder0 = encoder_block(inputs, 32) # 128\n",
        "\tencoder1_pool, encoder1 = encoder_block(encoder0_pool, 64) # 64\n",
        "\tencoder2_pool, encoder2 = encoder_block(encoder1_pool, 128) # 32\n",
        "\tencoder3_pool, encoder3 = encoder_block(encoder2_pool, 256) # 16\n",
        "\tencoder4_pool, encoder4 = encoder_block(encoder3_pool, 512) # 8\n",
        "\tcenter = conv_block(encoder4_pool, 1024) # center\n",
        "\tdecoder4 = decoder_block(center, encoder4, 512) # 16\n",
        "\tdecoder3 = decoder_block(decoder4, encoder3, 256) # 32\n",
        "\tdecoder2 = decoder_block(decoder3, encoder2, 128) # 64\n",
        "\tdecoder1 = decoder_block(decoder2, encoder1, 64) # 128\n",
        "\tdecoder0 = decoder_block(decoder1, encoder0, 32) # 256\n",
        "\toutputs = layers.Conv2D(1, (1, 1), activation='sigmoid')(decoder0)\n",
        "\n",
        "\tmodel = models.Model(inputs=[inputs], outputs=[outputs])\n",
        "\n",
        "\tmodel.compile(\n",
        "\t\toptimizer=optimizers.get(OPTIMIZER),\n",
        "\t\tloss=losses.get(LOSS),\n",
        "\t\tmetrics=[metrics.get(metric) for metric in METRICS])\n",
        "\n",
        "\treturn model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uu_E7OTDBCoS"
      },
      "source": [
        "# Training the model\n",
        "\n",
        "Now we train the compiled network by calling `.fit()`.  Typically we would train at least a full epoch, which is a full pass through the trainind dataset. However, this takes prohibitively long when using Google Clab + Google Drive for storage, an epoch takes 10 minutes on a T4 GPU. We will instead train for 2 reduced epochs of step size 20, meaning we will train on 40 samples, which works for a demo.  On a faster infrasturcture setup, the model may improve with more iteration, which can be experimented with by [hyperparameter tuning](https://cloud.google.com/ml-engine/docs/tensorflow/using-hyperparameter-tuning) and implementation of early stopping mechanisms.\n",
        "\n",
        "Notice in the Google Colab Resource usage window how long it takes for GPU memory to start being utilized. it's helpful during training to inspect the GPU and CPU resource usage in real time to understand what resources your model training needs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NzzaWxOhSxBy"
      },
      "outputs": [],
      "source": [
        "m = get_model()\n",
        "m.fit(\n",
        "    x=training,\n",
        "    epochs=EPOCHS,\n",
        "    steps_per_epoch=20, # typically we would specify int(TRAIN_SIZE / BATCH_SIZE) to iterate through all batches\n",
        "    validation_data=validation,\n",
        "    validation_steps=VAL_SIZE)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9GrXvK7i6B_"
      },
      "source": [
        "Let's save the trained model to file. We need to switch back to the user outputs dir briefly in order to save to a writeable user folder in your personal Google Drive."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd $user_outputs_dir"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "11aiXtSsfBSC"
      },
      "outputs": [],
      "source": [
        "save_model_path = os.path.join(f\"{FOLDER}/model_out_batch_{BATCH_SIZE}_ep{EPOCHS}/\")\n",
        "if (not os.path.isdir(save_model_path)):\n",
        "  os.mkdir(save_model_path)\n",
        "m.save(save_model_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We will switch back to the already processed outputs to load a model trained for 16 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%cd $processed_outputs_dir"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U2XrwZHp66j4"
      },
      "source": [
        "After saving the trained model, you may want to run predictions later. Let's try loading our saved model to show how it can be reused without having to retrain the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-RJpNfEUS1qp"
      },
      "outputs": [],
      "source": [
        "# Load a trained model.\n",
        "MODEL_DIR = f\"{FOLDER}/model_out_batch_{BATCH_SIZE}_ep{EPOCHS}/\"\n",
        "m = tf.keras.models.load_model(MODEL_DIR)\n",
        "m.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J1ySNup0xCqN"
      },
      "source": [
        "# Prediction\n",
        "\n",
        "Now, let's make predictions for a new area to see how the model generalizes to new data. Bear in mind, the model was trained on data in the US because that is where the labels were available, but let's see the trained model applies to a region of Lima, Peru.\n",
        "\n",
        "Again, we will use a defined geometry to process and export imagery from Earth Engine in TFRecord format. Then we'll use our trained model to predict impervious surface area percentages on the new imagery and write the predictions to both a TFRecord file and image patches (true color and prediction).\n",
        "\n",
        "We separate the image export from the predict function because the export only needs to happen once, but perhaps you'll experiment with the model setup and run new predictions several times."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3WDAa-RUpXP"
      },
      "outputs": [],
      "source": [
        "def doExport(out_image_base, kernel_buffer, region):\n",
        "  \"\"\"Run the image export task.  Block until complete.\n",
        "  \"\"\"\n",
        "\n",
        "  task = ee.batch.Export.image.toDrive(\n",
        "    image = image.select(BANDS),\n",
        "    description = out_image_base,\n",
        "    folder = FOLDER,\n",
        "    fileNamePrefix = f\"{out_image_base}\",\n",
        "    region = region.getInfo()['coordinates'],\n",
        "    scale = 30,\n",
        "    fileFormat = 'TFRecord',\n",
        "    maxPixels = 1e10,\n",
        "    formatOptions = {\n",
        "      'patchDimensions': KERNEL_SHAPE,\n",
        "      'kernelSize': kernel_buffer,\n",
        "      'compressed': True,\n",
        "      'maxFileSize': 104857600\n",
        "    }\n",
        "  )\n",
        "\n",
        "  task.start()\n",
        "\n",
        "  # Block until the task completes.\n",
        "  print('Running image export to google drive...')\n",
        "  import time\n",
        "  while task.active():\n",
        "    time.sleep(30)\n",
        "\n",
        "  # Error condition\n",
        "  if task.status()['state'] != 'COMPLETED':\n",
        "    print('Error with image export.')\n",
        "  else:\n",
        "    print('Image export completed.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zb_9_FflygVw"
      },
      "outputs": [],
      "source": [
        "def get_files_list(out_image_base):\n",
        "    \"\"\"Retrieve the list of image and JSON files.\"\"\"\n",
        "    filesList = glob.glob(f\"{FOLDER}/*\")\n",
        "    exportFilesList = [s for s in filesList if out_image_base in s]\n",
        "\n",
        "    imageFilesList = []\n",
        "    jsonFile = None\n",
        "    for f in exportFilesList:\n",
        "        if f.endswith('.tfrecord.gz'):\n",
        "            imageFilesList.append(f)\n",
        "        elif f.endswith('.json'):\n",
        "            jsonFile = f\n",
        "            \n",
        "    imageFilesList.sort()\n",
        "    \n",
        "    return imageFilesList, jsonFile\n",
        "\n",
        "def load_json(jsonFile):\n",
        "    \"\"\"Load the JSON mixer file.\"\"\"\n",
        "    jsonText = !cat {jsonFile}\n",
        "    mixer = json.loads(jsonText.nlstr)\n",
        "    return mixer\n",
        "\n",
        "def save_predictions(predictions, patches, x_buffer, y_buffer):\n",
        "    \"\"\"Process and save the predictions.\"\"\"\n",
        "    for i, prediction in zip(range(len(predictions)), predictions):\n",
        "        predictionPatch = prediction[\n",
        "            x_buffer:x_buffer+KERNEL_SIZE, y_buffer:y_buffer+KERNEL_SIZE]\n",
        "        p_image_array = np.array(predictionPatch)\n",
        "        tifffile.imsave(f\"{FOLDER}/{PREDICTIONS}/patch_pred_{i}.tif\", p_image_array)\n",
        "\n",
        "def get_image_features_dict(bands, buffered_shape):\n",
        "    \"\"\"Generate the image features dictionary.\"\"\"\n",
        "    imageColumns = [\n",
        "        tf.io.FixedLenFeature(shape=buffered_shape, dtype=tf.float32) for k in bands\n",
        "    ]\n",
        "    imageFeaturesDict = dict(zip(bands, imageColumns))\n",
        "    return imageFeaturesDict\n",
        "\n",
        "def process_dataset(imageFilesList, imageFeaturesDict):\n",
        "    \"\"\"Process the image dataset.\"\"\"\n",
        "    def parse_image(example_proto):\n",
        "        return tf.io.parse_single_example(example_proto, imageFeaturesDict)\n",
        "\n",
        "    def toTupleImage(inputs):\n",
        "        inputsList = [inputs.get(key) for key in BANDS]\n",
        "        stacked = tf.stack(inputsList, axis=0)\n",
        "        stacked = tf.transpose(stacked, [1, 2, 0])\n",
        "        return stacked\n",
        "\n",
        "    imageDataset = tf.data.TFRecordDataset(imageFilesList, compression_type='GZIP')\n",
        "    imageDataset = imageDataset.map(parse_image, num_parallel_calls=5)\n",
        "    imageDataset = imageDataset.map(toTupleImage).batch(1)\n",
        "    \n",
        "    return imageDataset\n",
        "\n",
        "def doPrediction(out_image_base, kernel_buffer, save_prediction = False):\n",
        "    \"\"\"Perform inference on exported imagery, upload to Earth Engine.\"\"\"\n",
        "    # Assuming BANDS, KERNEL_SHAPE, and FOLDER are defined globally or can be set here\n",
        "    \n",
        "    # 1. Calculate required parameters\n",
        "    x_buffer = int(kernel_buffer[0] / 2)\n",
        "    y_buffer = int(kernel_buffer[1] / 2)\n",
        "    buffered_shape = [\n",
        "        KERNEL_SHAPE[0] + kernel_buffer[0],\n",
        "        KERNEL_SHAPE[1] + kernel_buffer[1]\n",
        "    ]\n",
        "    \n",
        "    # 2. Retrieve the list of files\n",
        "    imageFilesList, jsonFile = get_files_list(out_image_base)\n",
        "\n",
        "    # 3. Generate the image features dictionary\n",
        "    imageFeaturesDict = get_image_features_dict(BANDS, buffered_shape)\n",
        "\n",
        "    # 4. Process the image dataset\n",
        "    imageDataset = process_dataset(imageFilesList, imageFeaturesDict)\n",
        "\n",
        "    # 5. Perform predictions\n",
        "    mixer = load_json(jsonFile)\n",
        "    patches = mixer['totalPatches']\n",
        "    predictions = m.predict(imageDataset, steps=patches, verbose=1)\n",
        "\n",
        "    # 6. Save the predictions\n",
        "    if save_prediction:\n",
        "      save_predictions(predictions, patches, x_buffer, y_buffer)\n",
        "\n",
        "    return predictions, patches"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LZqlymOehnQO"
      },
      "source": [
        "Let's supply our area of interest to make predictions for. We also provide some string parameters for file naming and finally, the shape for the image outputs. On that, the model can accept larger dimensions than 256x256 (which it was trained on) provided that they are uniform in width and height (note that we didn't specify an input shape in the first layer of the network) but at some point as the dimensions increase a memory ceiling will be encountered ([reference](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Long_Fully_Convolutional_Networks_2015_CVPR_paper.pdf)). So, proceed with caution on that. We will try one technique here to address the common issue of edge artifacts. Specifically, we will buffer our images during prediction using a 128x128 kernel, which pads the image with an additional 64 pixels on both width and height, and then clip the prediction down to the original central region of 256x256 pixels."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LoGESVQ_OvHC"
      },
      "outputs": [],
      "source": [
        "# Output assets\n",
        "\n",
        "# Base file name to use for TFRecord files and assets.\n",
        "lima_image_base = 'FCNN_demo_lima_384_'\n",
        "# Half this will extend on the sides of each patch.\n",
        "lima_kernel_buffer = [128, 128]\n",
        "# Lima [-77.133581 -12.164808 -76.876993 -11.93859]\n",
        "lima_region = ee.Geometry.Polygon(\n",
        "        [[[-77.133581, -11.93859],\n",
        "          [-77.133581, -12.164808],\n",
        "          [-76.876993, -12.164808],\n",
        "          [-76.876993, -11.93859]]], None, False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Exporting the image we ran prediction takes too long with Colab + Google Drive, so we'll comment this out for now."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lLNEOLkXWvSi"
      },
      "outputs": [],
      "source": [
        "# Run the export.\n",
        "# doExport(lima_image_base, lima_kernel_buffer, lima_region)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KxACnxKFrQ_J"
      },
      "outputs": [],
      "source": [
        "# Run the prediction.\n",
        "predictions, patches = doPrediction(lima_image_base, lima_kernel_buffer, save_prediction=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uj_G9OZ1xH6K"
      },
      "source": [
        "# Display the output\n",
        "\n",
        "Let's take a look at randomly indexed samples from our prediction output. We also will check the minimum and maximum values (impervious surface area percentages) in the prediction image."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ju2l6gRKwnv6"
      },
      "outputs": [],
      "source": [
        "import matplotlib.gridspec as gridspec\n",
        "\n",
        "# Define a gridspec layout to keep the left and right plot the same size\n",
        "fig = plt.figure(figsize=(12, 6))\n",
        "gs = gridspec.GridSpec(1, 3, width_ratios=[1, 1, 0.05])\n",
        "\n",
        "ax0 = plt.subplot(gs[0])\n",
        "ax1 = plt.subplot(gs[1])\n",
        "cax = plt.subplot(gs[2])\n",
        "\n",
        "index_random = randrange(1) # randomly pick different integers\n",
        "print(index_random)\n",
        "\n",
        "ax0.imshow(np.array(Image.open(f\"{FOLDER}/{TEST_PATCHES}/patch_test_{index_random}.tif\")))\n",
        "ax0.set_title(f'RGB').set_fontsize(14)\n",
        "\n",
        "# Show the predicted image with a colormap (e.g., 'viridis') which indicates probability.\n",
        "im = ax1.imshow(np.array(Image.open(f\"{FOLDER}/{PREDICTIONS}/patch_pred_{index_random}.tif\")), cmap='viridis', vmin=0, vmax=1)\n",
        "ax1.set_title(f'Predicted impervious surface area percentage').set_fontsize(14)\n",
        "\n",
        "# Add a colorbar for the prediction image in its own axis\n",
        "cbar = fig.colorbar(im, cax=cax, orientation='vertical')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oW6CHqsYcn3W"
      },
      "outputs": [],
      "source": [
        "image_test = np.array(Image.open(f\"{FOLDER}/{PREDICTIONS}/patch_pred_{index_random}.tif\"))\n",
        "image_test.min(), image_test.max()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "private_outputs": true,
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
